{
    "contents" : "Going Beyond RFM - CLV Prediction\n========================================================\nauthor: Ashish Kulkarni\ndate: \n\nIntroduction\n========================================================\n<b>Customer Lifetime Value (CLV):</b> Present value of the future cash flows associated with a customer.\n\n- Some of the earlier work computed <i>customer profitability</i> based on their prior behavior.\n- But CLV is all about looking into the future, not the past.\n- This requires us to accurately <i>model</i> for future revenues and user fallout.\n\nA Stochastic Modeling Approach\n========================================================\n![latent](latent.png)\n\n- Observations are a realization of latent traits\n- Use Bayes' theorem to estimate a user's latent traits as a function of observations\n- Then predict future behavior as a function of these latent traits\n\nCLV Prediction using RFM\n========================================================\nAssumes monetary value to be independent of the underlying transaction process.\n\n$$CLV = margin \\times revenue/transaction \\times DET$$\n\nwhere, DET (Discounted Expected Transactions) is the present value of the future transaction flow.\n\nNext, we develop the model to predict discounted expected number of transactions.\n\nPredicting Expected Number of Transactions\n========================================================\n<br/>\n             \nPredicting expected number of transactions for a user involves simulation of two events -\n\n![User survival (or churn)](coin_toss.jpg)\n***\n![number of trans](dice_roll.jpg)\n- User survival (or churn): think coin toss\n- Number of transactions done while the user is active: think dice roll\n\nModel for DET prediction - The Pareto/NBD Model\n========================================================\n<small>\nBased on five assuptions:\n\n- While active, the number of transactions made by a user in a time period $t$ follows a Poisson distribution with transaction rate $\\lambda$.\n- Heterogeneity in transaction rates across users follows a gamma distribution with shape parameter $r$ and scale parameter $\\alpha$.\n- Each user has an unobserved ``lifetime'' $\\tau$. This point at which the user churns is distributed exponentially with dropout rate $\\mu$.\n- The prior on dropout rate follows a gamma distribution with shape parameter $s$ and scale parameter $\\beta$.\n- The transaction rate $\\lambda$ and dropout rate $\\mu$ vary independently across customers.\n</small>\n\nModel Development\n========================================================\n<small>\nRequires only two pieces of input about each user's past purchasing history:\n- recency: time from now of occurrence of last transaction\n- frequency: number of transactions made during a specified time period\n\nThis is expressed using the notation $(X = x, t_x, T)$, where $x$ is the number of observed transactions in the time period $(0, T]$ and $t_x (0 < t_x \\leq T)$, is the time of last transaction.\n\n$$DET(\\delta|r,\\alpha,s,\\beta,X=x,t_x,T) = \\frac{\\alpha^r\\beta^s\\delta^{s-1}\\Gamma(r+x+1)\\Psi[s,s;\\delta(\\beta+T)]}{\\Gamma(r)(\\alpha+T)^{r+x+1}L(r,\\alpha,s,\\beta|X=x,t_x,T)}$$\nwhere, $r,\\alpha,s,\\beta$ are the model parameters, $\\Psi(.)$ is the confluent hypergeometric function of the second kind; $\\delta$ is the continuously compounded rate of interest and $L(.)$ is the Pareto/NBD likelihood function.\n</small>\n\nComments about Pareto/NBD Model\n========================================================\n<small>\n- The Pareto/NBD model does quite well in modeling the user behavior.\n- The emperical performance of the model is excellent and well proven. However, the model suffers from the following drawbacks.\n- The likelihood function associated with the Pareto/NBD model is quite complex, involving numerous evaluations of the Gaussian hypergeometric function.\n- Multiple evaluations of the Gaussian hypergeometric function are compuatationally intensive.\n</small>\n\nModel for DET Prediction - BG/NBD Model\n========================================================\n<small>\nBased on following five assumptions:\n- While active, the number of transactions made by a user follows a Poisson distribution with transaction rate $\\lambda$.\n- Heterogeneity in $\\lambda$ follows a gamma distribution with shape parameter $r$ and scale parameter $\\alpha$.\n- After any transaction, a user becomes inactive with probability $p$, so, $P$(inactive immediately after $j^{th}$ transaction) $= p(1-p)^{j-1}, j = 1,2,3,...$.\n- Heterogeneity in $p$ follows a beta distribution, $p \\sim Beta(a,b)$.\n- The transaction rate $\\lambda$ and churn probability $p$ vary independently across users.\n</small>\n\nModel Development\n========================================================\n<small>\nLikelihood function for a randomly chosen user with purchase history $(X=x,t_x,T)$\n$$\n  \\begin{aligned}\n  L(r,\\alpha,a,b|X=x,t_x,T) &= \\frac{B(a,b+x)}{B(a,b)}\\frac{\\Gamma(r+x)\\alpha^r}{\\Gamma(r)(\\alpha+T)^{r+x}} \\\\ \n  &+ \\delta_{x>0}\\frac{B(a+1,b+x-1)}{B(a,b)}\\frac{\\Gamma(r+x)\\alpha^r}{\\Gamma(r)(\\alpha+t_x)^{r+x}}\n  \\end{aligned}\n$$\nThe model parameters can be estimated using maximum likelihood estimation. For a sample of $N$ users, where user $i$ had $X_i=x_i$ transactions in the period $(0,T_i]$, with recency $t_{x_i}$, the sample log likelihood is given as:\n$$LL(r,\\alpha,a,b) = \\sum_{i=1}^N ln[L(r,\\alpha,a,b|X_i=x_i,t_{x_i},T_i)]$$\nThis can be maximized using standard numerical optimization routines\n</small>\n\nExpression for DET prediction\n========================================================\n<small>\n$$\n  \\begin{aligned}\n  E[Y(t)|X &= x,t_x,T,r,\\alpha,a,b] = \\\\\n   &\\frac{\\frac{a+b+x-1}{a-1}[1-(\\frac{\\alpha+T}{\\alpha+T+t})^{r+x} {}_{2}F_{1}(r+x,b+x;a+b+x-1;\\frac{t}{\\alpha+T+t})]}{1+\\delta_{x>0}\\frac{a}{b+x-1}(\\frac{\\alpha+T}{\\alpha+t_x})^{r+x}}\n  \\end{aligned}\n$$\n\n- Requires single evaluation of the Gaussian hypergeometric function for any user. Remainder of the expression is simple arithmetic.\n- Thus compared to Pareto/NBD, BG/NBD is computationally efficient.\n- Performance of this model is almost at-par with the Pareto/NBD model.\n</small>\n\nData Preparation: Query\n=========================================================\n- For the merchant of interest and for a given period <t1, t2>, query for transactions for each user\n- Compute the following:\n- Recency (R): Difference between the date of last transaction done by user and the end of period\n- Frequency (F): Number of transactions done by the user during the period\n- Monetary Valur (M): Average spending amount\n- Date of first transaction (start)\n- Date of last transaction (end)\n\nData Preparation: Customer-by-sufficient-statistic (CBS)\n==========================================================\nFor the DET model, we use \n- Recency (R)\n- Frequency (F)\n- Period of observation: Difference between date of first transaction (start) and the end of period (t2)\n- Create a CBS matrix where a row corresponds to each user and the columns correspond to each of the values above\n\nDET Model: Dataset\n=====================================================================\n- Bank transactions done at Dillards during the period 2014-01-01 through 2014-12-31 are used for training the DET model\n- Bank transactions done at Dillards duging the period 2015-01-01 through 2015-03-31 are used for model validation\n- Gather sufficient statistic from training/validation data, where, sufficient statistic is frequency of transaction, recency and total time observed for each user.\n- Training size: 473,356\n- Validation size: 151,772\n\nDET Model: Parameters\n====================================================================\nMLE estimation of Pareto/NBD model params, trained using a 50,000 training sample\n- $r$, shape param for transaction rate Gamma distribution: 1.205\n- $\\alpha$, scale param for transaction rate Gamma distribution: 9.945\n- $s$, shape param for dropout rate Gamma distribution: 0.121\n- $\\beta$, scale param for dropout rate Gamma distribution: 0.061\n\nDET Model: Transaction Rate heterogeneity\n===================================================================\n```{r, echo=FALSE}\nlibrary(BTYD)\nparams <- c(1.20530345, 9.94580922, 0.12131091, 0.06112537)\n#pnbd.PlotTransactionRateHeterogeneity(params, lim = NULL)\nx  <- seq(0.0,0.9,length=100)\nhx <- dgamma(x, shape=params[1], rate=params[2], log=FALSE)\nplot(x, hx, type=\"l\", xlab=\"Transaction Rate\", ylab=\"Density\")\n```\n\nDET Model: Dropout Rate heterogeneity\n===================================================================\n```{r, echo=FALSE}\nx  <- seq(0,20,length=100)\nhx <- dgamma(x, shape=params[3], rate=params[4], log=FALSE)\nplot(x, hx, type=\"l\", xlab=\"Dropout Rate\", ylab=\"Density\")\n```\n\nDET Model: Model Evaluation on Validation Set\n===================================================================\n```{r, echo=FALSE}\nsource(\"clv_header_new.R\")\npreds    <- pnbd.ConditionalExpectedTransactions(params, T.star=12.71, data.train[, 'x'], data.train[, 't.x'], data.train[, 'T.cal'])\npreds.df <- data.frame(id=data.train[, 'id'], p=preds)\ncompare.trans <- merge(data.test[, c('id', 'x.star')], preds.df)\ncompare.trans <- na.omit(compare.trans)\ndata.validation <- merge(data.train, data.test[, c('id', 'x.star')])\ndata.validation <- merge(data.validation, compare.trans)\ndata.validation <- na.omit(data.validation)\nPlotConditionalExpectedFrequency(data.validation, censor=10)\n```\n\nDET Model: Model Evaluation on Validation Set\n===================================================================\n- RMSE: \n```{r, echo=FALSE}\nrmse(act=compare.trans[, \"x.star\"], est=compare.trans[, \"p\"])\n```\n\n```{r, echo=FALSE}\n#msle(act=compare.trans[, \"x\"], est=compare.trans[, \"p\"])\n```\n\nCLV Computation\n==================================================================\nCLV for a user is calculated as - \n\nCLV = DET * M * Margin\n\n- $\\delta$ used in CLV computation is set to 0.0027 (annual discount rate of 15% compounded continuously)\n- Margin is assumed to be 30%\n\nReferences\n===================================================================\n[1] Fader, P. S., Hardie, B. G. S., & Lee, K. L. (2005). RFM and CLV: Using Iso-Value Curves for Customer Base Analysis. Journal of Marketing Research, 42(November), 415–430. doi:10.1509/jmkr.2005.42.4.415\n\n[2] Fader, P. S., Hardie, B. G. S., & Lee, K. L. (2005). ?Counting Your Customers? the Easy Way: An Alternative to the Pareto/NBD Model. Marketing Science, 24(2), 275–284. doi:10.1287/mksc.1040.0098\n",
    "created" : 1428391350686.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3386552683",
    "id" : "96A96D52",
    "lastKnownWriteTime" : 1435919204,
    "path" : "C:/Users/akulkarni/RWorkspace/rfm/rfm_clv.Rpres",
    "project_path" : "rfm_clv.Rpres",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_presentation"
}